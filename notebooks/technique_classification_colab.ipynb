{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ptc_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhzJjbL9xIlDYTBGauR4+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asakhala921/Propoganda-Detection-in-News-Articles/blob/master/notebooks/technique_classification_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iw9ct4LYr1J"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "vofp_0pb3WSv",
        "outputId": "152b7279-cd06-491a-8630-de93f5ab9a6a"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "!unzip -q ptc_corpus.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-04207c85-439f-48c5-9faf-dce79b6ee7e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-04207c85-439f-48c5-9faf-dce79b6ee7e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ptc_corpus.zip to ptc_corpus.zip\n",
            "User uploaded file \"ptc_corpus.zip\" with length 4570646 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsCB1ZsG1EfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b46932-247b-487f-95ca-11e0976b4891"
      },
      "source": [
        "#@title Install Dependencies\n",
        "%%bash\n",
        "git clone -b add_articleid_to_semeval2020_11 https://github.com/hemildesai/datasets.git\n",
        "git clone https://github.com/asakhala921/Propoganda-Detection-in-News-Articles.git\n",
        "cd Propoganda-Detection-in-News-Articles && pip install -qU -r requirements.txt\n",
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/Propoganda-Detection-in-News-Articles\n",
            "Installing collected packages: ucla263brah\n",
            "  Running setup.py develop for ucla263brah\n",
            "Successfully installed ucla263brah\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'datasets'...\n",
            "Cloning into 'Propoganda-Detection-in-News-Articles'...\n",
            "ERROR: pandas-profiling 2.11.0 has requirement requests>=2.24.0, but you'll have requests 2.23.0 which is incompatible.\n",
            "ERROR: pandas-profiling 2.11.0 has requirement tqdm>=4.48.2, but you'll have tqdm 4.41.1 which is incompatible.\n",
            "ERROR: phik 0.11.2 has requirement scipy>=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Jku7xB1R9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b0de6d-fc42-44a0-d2d7-24954ce0b233"
      },
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "import torch\n",
        "from torch.utils import data as D\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 100\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "import ucla263brah.tc as tc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mDqAOSMYwKE"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k60uo34s1MH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b3a755-fb3f-4bb9-97ce-e5f01fc94058"
      },
      "source": [
        "dataset = load_dataset(\"./datasets/datasets/sem_eval_2020_task_11/\", data_dir=\"ptc_corpus/datasets/\")\n",
        "labels, colors = tc.utils.get_labels_and_colors(dataset)\n",
        "\n",
        "tokenizer = tc.dataset.get_tokenizer(\"bert-base-uncased\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-38c395c26995e028\n",
            "Reusing dataset sem_eval_2020_task_11 (/root/.cache/huggingface/datasets/sem_eval_2020_task_11/default-38c395c26995e028/1.1.0/39cf0fe55e04b30029866806211d306797c07efd265a5c4fa33cfe512adf142e)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B9L7nYKsMAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf07aa6-06b1-46e8-c943-54309dd3de29"
      },
      "source": [
        "df, ldf, mismatch = tc.utils.prepare_df(dataset, tokenizer, labels)\n",
        "profile1 = tc.utils.generate_report(df)\n",
        "profile2 = tc.utils.generate_report(ldf)\n",
        "\n",
        "profile1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ4G6NpA43yD"
      },
      "source": [
        "html = tc.utils.visualize_example(1, dataset, labels, colors)\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3XPoArYY2j4"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5xLNoCb8dTU"
      },
      "source": [
        "train_data, dev_data, test_data = tc.dataset.prepare_data(dataset, max_length=256, prepare_fn=tc.dataset.get_spans_in_context)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=random.seed(SEED), stratify=list(map(lambda x: x[\"technique\"], train_data)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TV1Nf7jVWXz"
      },
      "source": [
        "## Train and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx95nImUaMCd"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "collate_fn = partial(tc.trainer.collate_fn_with_tokenizer, tokenizer=tokenizer)\n",
        "train_dataset = tc.dataset.PTCTechniqueClassificationDataset(train_data, tokenizer)\n",
        "val_dataset = tc.dataset.PTCTechniqueClassificationDataset(val_data, tokenizer)\n",
        "dev_dataset = tc.dataset.PTCTechniqueClassificationDataset(dev_data, tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n",
        "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=1e-4,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    label_smoothing_factor=0.10\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n",
        "trainer = tc.trainer.PTCTechniqueClassificationTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    collate_fn=collate_fn\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "preds = predict_and_print_results(trainer, dev_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q7eUwfLCuyO"
      },
      "source": [
        "tokenizer = get_tokenizer(\"roberta-large\")\n",
        "train_dataset = PTCTechniqueClassificationDataset(train_data, tokenizer)\n",
        "val_dataset = PTCTechniqueClassificationDataset(val_data, tokenizer)\n",
        "dev_dataset = PTCTechniqueClassificationDataset(dev_data, tokenizer)\n",
        "collate_fn = partial(collate_fn_with_tokenizer, tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=1e-2,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True\n",
        "    # label_smoothing_factor=0.10\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=len(labels))\n",
        "trainer = PTCTechniqueClassificationTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    collate_fn=collate_fn\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "preds = predict_and_print_results(trainer, dev_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "kSb1Joz3HE9x",
        "outputId": "026e24da-940e-48a5-e9f2-908bd8c9ba7b"
      },
      "source": [
        "tokenizer = tc.dataset.get_tokenizer(\"microsoft/deberta-large\")\n",
        "train_dataset = tc.dataset.PTCTechniqueClassificationDataset(train_data, tokenizer)\n",
        "val_dataset = tc.dataset.PTCTechniqueClassificationDataset(val_data, tokenizer)\n",
        "dev_dataset = tc.dataset.PTCTechniqueClassificationDataset(dev_data, tokenizer)\n",
        "collate_fn = partial(tc.trainer.collate_fn_with_tokenizer, tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,  # batch size per device during training\n",
        "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=1e-2,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    label_smoothing_factor=0.10,\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-large\", \n",
        "    num_labels=len(labels),\n",
        "    label2id = {b: a for a, b in enumerate(labels)},\n",
        "    id2label = {a: b for a, b in enumerate(labels)},\n",
        ")\n",
        "\n",
        "trainer = tc.trainer.PTCTechniqueClassificationTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    collate_fn=collate_fn\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "preds = tc.predictor.predict_and_print_results(trainer, dev_dataset, labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'config']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2001' max='2067' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2001/2067 1:26:18 < 02:50, 0.39 it/s, Epoch 2.90/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.342200</td>\n",
              "      <td>1.553379</td>\n",
              "      <td>97.355200</td>\n",
              "      <td>6.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>1.388748</td>\n",
              "      <td>97.817100</td>\n",
              "      <td>6.267000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2067' max='2067' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2067/2067 1:32:06, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.342200</td>\n",
              "      <td>1.553379</td>\n",
              "      <td>97.355200</td>\n",
              "      <td>6.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>1.388748</td>\n",
              "      <td>97.817100</td>\n",
              "      <td>6.267000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.676500</td>\n",
              "      <td>1.327259</td>\n",
              "      <td>97.546600</td>\n",
              "      <td>6.284000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='532' max='532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [532/532 02:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "               Appeal_to_Authority     0.2857    0.2857    0.2857        14\n",
            "          Appeal_to_fear-prejudice     0.3265    0.3636    0.3441        44\n",
            "    Bandwagon,Reductio_ad_hitlerum     1.0000    0.6000    0.7500         5\n",
            "           Black-and-White_Fallacy     0.4286    0.1364    0.2069        22\n",
            "         Causal_Oversimplification     0.4706    0.4444    0.4571        18\n",
            "                             Doubt     0.5556    0.6061    0.5797        66\n",
            "         Exaggeration,Minimisation     0.4898    0.7059    0.5783        68\n",
            "                       Flag-Waving     0.8194    0.6782    0.7421        87\n",
            "                   Loaded_Language     0.7629    0.8215    0.7911       325\n",
            "             Name_Calling,Labeling     0.6933    0.8525    0.7647       183\n",
            "                        Repetition     0.6300    0.4345    0.5143       145\n",
            "                           Slogans     0.6897    0.5000    0.5797        40\n",
            "       Thought-terminating_Cliches     0.4000    0.2353    0.2963        17\n",
            "Whataboutism,Straw_Men,Red_Herring     0.4118    0.2414    0.3043        29\n",
            "\n",
            "                          accuracy                         0.6566      1063\n",
            "                         macro avg     0.5688    0.4932    0.5139      1063\n",
            "                      weighted avg     0.6538    0.6566    0.6458      1063\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xiwIyck8B6O",
        "outputId": "704ee199-6f99-42cd-a162-8de4541a931e"
      },
      "source": [
        "pred_df = tc.predictor.postprocess_predictions(dev_data, preds, labels)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "               Appeal_to_Authority     0.3077    0.2857    0.2963        14\n",
            "          Appeal_to_fear-prejudice     0.3265    0.3636    0.3441        44\n",
            "    Bandwagon,Reductio_ad_hitlerum     1.0000    0.6000    0.7500         5\n",
            "           Black-and-White_Fallacy     0.4286    0.1364    0.2069        22\n",
            "         Causal_Oversimplification     0.4706    0.4444    0.4571        18\n",
            "                             Doubt     0.6061    0.6061    0.6061        66\n",
            "         Exaggeration,Minimisation     0.4898    0.7059    0.5783        68\n",
            "                       Flag-Waving     0.8358    0.6437    0.7273        87\n",
            "                   Loaded_Language     0.7934    0.8154    0.8042       325\n",
            "             Name_Calling,Labeling     0.7327    0.8087    0.7688       183\n",
            "                        Repetition     0.6490    0.6759    0.6622       145\n",
            "                           Slogans     0.6897    0.5000    0.5797        40\n",
            "       Thought-terminating_Cliches     0.4000    0.2353    0.2963        17\n",
            "Whataboutism,Straw_Men,Red_Herring     0.4118    0.2414    0.3043        29\n",
            "\n",
            "                          accuracy                         0.6773      1063\n",
            "                         macro avg     0.5815    0.5045    0.5273      1063\n",
            "                      weighted avg     0.6773    0.6773    0.6713      1063\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "PnA31SRtPrhw",
        "outputId": "d8b89302-42ad-454d-d8fa-2ae5e4fb4671"
      },
      "source": [
        "tokenizer = tc.dataset.get_tokenizer(\"microsoft/deberta-large\")\n",
        "final_dataset = tc.dataset.PTCTechniqueClassificationDataset(train_data + val_data + dev_data, tokenizer)\n",
        "val_dataset = tc.dataset.PTCTechniqueClassificationDataset(val_data, tokenizer)\n",
        "collate_fn = partial(tc.trainer.collate_fn_with_tokenizer, tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,  # batch size per device during training\n",
        "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=1e-2,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-5,\n",
        "    label_smoothing_factor=0.10,\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-large\", \n",
        "    num_labels=len(labels),\n",
        "    label2id = {b: a for a, b in enumerate(labels)},\n",
        "    id2label = {a: b for a, b in enumerate(labels)},\n",
        ")\n",
        "\n",
        "trainer = tc.trainer.PTCTechniqueClassificationTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=final_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    collate_fn=collate_fn\n",
        "    # compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "test_dataset = tc.dataset.PTCTechniqueClassificationDataset(test_data, tokenizer)\n",
        "preds_test = tc.predictor.predict_and_print_results(trainer, test_dataset, labels)\n",
        "test_df = tc.predictor.postprocess_predictions(test_data, preds_test, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'config']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='140' max='2697' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 140/2697 05:22 < 1:39:32, 0.43 it/s, Epoch 0.15/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k1d7jJYIYTH"
      },
      "source": [
        "lines = [f\"{row.article_ids[7:]}\\t{labels[row.predictions]}\\t{row.start}\\t{row.end}\\n\" for _, row in test_df.iterrows()]\n",
        "\n",
        "with open(\"./test_set_tc_output.out\", \"w\") as f:\n",
        "     f.writelines(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t581xIWFVnRX"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "5ZO9E4FG_rs9",
        "cellView": "form",
        "outputId": "dc83ebc4-bc31-4644-8b05-8016198bb491"
      },
      "source": [
        "#@title Setup Colab SSH\n",
        "\n",
        "!pip install colab_ssh --upgrade\n",
        "\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "launch_ssh_cloudflared(password=\"YOLO123\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colab_ssh\n",
            "  Downloading https://files.pythonhosted.org/packages/74/c0/a2d6cc985d9496968b80203105f20c8d3845effa45dd4cb46c22879f1e44/colab_ssh-0.3.15-py3-none-any.whl\n",
            "Installing collected packages: colab-ssh\n",
            "Successfully installed colab-ssh-0.3.15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "*{\n",
              "\toutline:none;\n",
              "}\n",
              "code{\n",
              "\tdisplay:inline-block;\n",
              "\tpadding:5px 10px;\n",
              "\tbackground: #444;\n",
              "\tborder-radius: 4px;\n",
              "\twhite-space: pre-wrap;\n",
              "\tposition:relative;\n",
              "\tcolor:white;\n",
              "}\n",
              ".copy-code-button{\n",
              "\tfloat:right;\n",
              "\tbackground:#333;\n",
              "\tcolor:white;\n",
              "\tborder: none;\n",
              "\tmargin: 0 0 0 10px;\n",
              "\tcursor: pointer;\n",
              "}\n",
              "p, li{\n",
              "\tmax-width:700px;\n",
              "}\n",
              ".choices{\n",
              "\tdisplay:flex;\n",
              "\tflex: 1 0 auto;\n",
              "}\n",
              ".choice-section{\n",
              "\tborder:solid 1px #555;\n",
              "\tborder-radius: 4px;\n",
              "\tmin-width:300px;\n",
              "\tmargin: 10px 15px 0 0;\n",
              "\tpadding: 0 15px 15px 15px ;\n",
              "}\n",
              ".button{\n",
              "\tpadding: 10px 15px;\n",
              "\tbackground:#333;\n",
              "\tborder-radius: 4px;\n",
              "\tborder:solid 1px #555;\n",
              "\tcolor:white;\n",
              "\tfont-weight:bold;\n",
              "\tcursor:pointer;\n",
              "}\n",
              ".pill{\n",
              "\tpadding:2px 4px;\n",
              "\tborder-radius: 100px;\n",
              "\tbackground-color:#e65858;\n",
              "\tfont-size:12px;\n",
              "\tfont-weight:bold;\n",
              "\tmargin: 0 15px;\n",
              "\tcolor:white;\n",
              "}\n",
              "</style>\n",
              "<details class=\"choice-section\">\n",
              "\t<summary style=\"cursor:pointer\">\n",
              "\t\t<h3 style=\"display:inline-block;margin-top:15px\">⚙️ Client machine configuration<span class=\"pill\">Required</span></h3>\n",
              "\t</summary>\n",
              "\t<p>Don't worry, you only have to do this <b>once per client machine</b>.</p>\n",
              "\t<ol>\n",
              "\t\t<li>Download <a href=\"https://developers.cloudflare.com/argo-tunnel/getting-started/installation\">Cloudflared (Argo Tunnel)</a>, then copy the absolute path to the cloudflare binary</li>\n",
              "\t\t<li>Now, you have to append the following to your SSH config file (usually under ~/.ssh/config):</li>\n",
              "\t</ol>\n",
              "\t<code>Host *.trycloudflare.com\n",
              "\tHostName %h\n",
              "\tUser root\n",
              "\tPort 22\n",
              "\tProxyCommand &ltPUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&gt access ssh --hostname %h\n",
              "\t</code>\n",
              "</details>\n",
              "<div class=\"choices\">\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>SSH Terminal</h4>\n",
              "\t\t<p>To connect using your terminal, type this command:</p>\n",
              "\t\t<code>ssh caribbean-commonly-dominican-desperate.trycloudflare.com</code>\n",
              "\t</div>\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>VSCode Remote SSH</h4>\n",
              "\t\t<p>You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:</p>\n",
              "\t\t<code>caribbean-commonly-dominican-desperate.trycloudflare.com</code>\n",
              "\t</div>\n",
              "</div>\n",
              "\n",
              "<script>\n",
              "// Copy any string\n",
              "function fallbackCopyTextToClipboard(text) {\n",
              "  var textArea = document.createElement(\"textarea\");\n",
              "  textArea.value = text;\n",
              "  \n",
              "  // Avoid scrolling to bottom\n",
              "  textArea.style.top = \"0\";\n",
              "  textArea.style.left = \"0\";\n",
              "  textArea.style.position = \"fixed\";\n",
              "\n",
              "  document.body.appendChild(textArea);\n",
              "  textArea.focus();\n",
              "  textArea.select();\n",
              "\n",
              "  try {\n",
              "    var successful = document.execCommand('copy');\n",
              "    var msg = successful ? 'successful' : 'unsuccessful';\n",
              "    console.log('Fallback: Copying text command was ' + msg);\n",
              "  } catch (err) {\n",
              "    console.error('Fallback: Oops, unable to copy', err);\n",
              "  }\n",
              "\n",
              "  document.body.removeChild(textArea);\n",
              "}\n",
              "\n",
              "// Show the copy button with every code tag\n",
              "document.querySelectorAll('code').forEach(function (codeBlock) {\n",
              "\tconst codeToCopy= codeBlock.innerText;\n",
              "\tvar pre = document.createElement('pre');\n",
              "\tpre.innerText = codeToCopy;\n",
              "    var button = document.createElement('button');\n",
              "    button.className = 'copy-code-button';\n",
              "    button.type = 'button';\n",
              "    button.innerText = 'Copy';\n",
              "\tbutton.onclick = function(){\n",
              "\t\tfallbackCopyTextToClipboard(codeToCopy);\n",
              "\t\tbutton.innerText = 'Copied'\n",
              "\t\tsetTimeout(()=>{\n",
              "\t\t\tbutton.innerText = 'Copy'\n",
              "\t\t},2000)\n",
              "\t}\n",
              "\tcodeBlock.children = pre;\n",
              "\tcodeBlock.prepend(button)\n",
              "});\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF78Cm333E1h",
        "cellView": "form",
        "outputId": "e4982e42-77be-4e93-c048-07621879595e"
      },
      "source": [
        "#@title Upload logs to tensorboard.dev\n",
        "\n",
        "!tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Technique Classification with DeBerta Large\" \\\n",
        "  --description \"Training results from https://colab.research.google.com/drive/1eee0W3E4A1oRgeJypa2qVUGF1pO1hjHq#scrollTo=HF78Cm333E1h\" \\\n",
        "  --one_shot"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-09 12:43:32.187410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "./logs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=DjHQ3gprk178WF2KUKwOZjpLIa8kUb&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AY0e-g4ju8TWj28bttvHbW7FTwqmKLdbitGxRCM4bPCUr0L4UyTyEZFIVIA\n",
            "\n",
            "Data for the \"text\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/bTpSvTLNSayqBWGWUZ9NaQ/\n",
            "\n",
            "\u001b[1m[2021-03-09T12:43:58]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2021-03-09T12:43:59]\u001b[0m Total uploaded: 634 scalars, 5 tensors (3.0 kB), 0 binary objects\n",
            "\u001b[1m[2021-03-09T12:43:59]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/bTpSvTLNSayqBWGWUZ9NaQ/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxu_99RxBCpO"
      },
      "source": [
        "Tensorboard - https://tensorboard.dev/experiment/bTpSvTLNSayqBWGWUZ9NaQ/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7iMt6dr4YOW",
        "cellView": "form"
      },
      "source": [
        "#@title Save model to Huggingface Model Hub\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/hd10/semeval2020_task11_tc\n",
        "\n",
        "model.save_pretrained(\"./semeval2020_task11_tc\")\n",
        "tokenizer.save_pretrained(\"./semeval2020_task11_tc\")"
      ],
      "execution_count": 50,
      "outputs": []
    }
  ]
}